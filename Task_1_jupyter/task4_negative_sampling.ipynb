{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "task4_negative_sampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy5-6e_lOvn8",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1.4: Negative sampling (15 points)\n",
        "\n",
        "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
        "\n",
        "The task is to implement word2vec with negative sampling.\n",
        "\n",
        "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
        "\n",
        "$$ L = \\log\\sigma(u^T_o \\cdot u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j \\cdot u_c)]$$\n",
        "\n",
        "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
        "\n",
        "The first term calculates the similarity between positive examples (word from one window)\n",
        "\n",
        "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
        "$\\mathbb{E}_{j \\sim P(w)}$\n",
        "means that $j$ is distributed accordingly to unigram distribution.\n",
        "\n",
        "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
        "\n",
        "Useful links:\n",
        "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixl9QTadbPYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "from torch import cuda\n",
        "import random\n",
        "\n",
        "from numpy.random import multinomial\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_f17OrrfHVI",
        "colab_type": "code",
        "outputId": "4e84dc8b-d115-4c5a-c367-328ab5166a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "with open(\"/content/gdrive/My Drive/DeepPavlov/text8/text8\",\"r\") as f:\n",
        "  corpus = f.read()\n",
        "with open(\"/content/gdrive/My Drive/DeepPavlov/w2v_data.pkl\",\"rb\") as f:\n",
        "  [X, y, word2index, index2word] = pickle.load(f) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifYXRHhA7uoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 512\n",
        "window_size = X.shape[1]/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S8S03WPOxaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://programmer.group/pytorch-implements-word2vec.html\n",
        "# https://rguigoures.github.io/word2vec_pytorch/\n",
        "\n",
        "def sample_negative(corpus, sample_size=5):\n",
        "    sample_probability = OrderedDict()\n",
        "    word_counts = OrderedDict(Counter(corpus.split()).most_common(len(word2index)))\n",
        "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
        "    for word in word_counts:\n",
        "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
        "    words = list(word_counts.keys())\n",
        "    neg_samples = []\n",
        "    for i in range(X.shape[0]):\n",
        "        multi_df = multinomial(sample_size, list(sample_probability.values()))\n",
        "        sampled_index = np.where(multi_df>0)[0]\n",
        "        if len(sampled_index) < sample_size:\n",
        "          sampled_index = np.hstack((sampled_index, random.sample(range(len(word2index)), \\\n",
        "                                                                     int(sample_size - len(sampled_index)))))\n",
        "        assert len(sampled_index) == sample_size\n",
        "          \n",
        "        yield sampled_index\n",
        "\n",
        "neg_sampler = sample_negative(corpus, window_size*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBpeuK3GStze",
        "colab_type": "code",
        "outputId": "2a1b8ccb-779d-44b3-83c5-606d9c17ccfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17005197, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F01iIWvp6c3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping():\n",
        "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
        "        self.patience = patience\n",
        "        self.loss_list = []\n",
        "        self.min_percent_gain = min_percent_gain / 100.\n",
        "        \n",
        "    def update_loss(self, loss):\n",
        "        self.loss_list.append(loss)\n",
        "        if len(self.loss_list) > self.patience:\n",
        "            del self.loss_list[0]\n",
        "    \n",
        "    def stop_training(self):\n",
        "        if len(self.loss_list) == 1:\n",
        "            return False\n",
        "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
        "        print(\"Loss gain: {}%\".format(round(100*gain,2)))\n",
        "        if gain < self.min_percent_gain:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "class My_Dataset(Dataset):\n",
        "    def __init__(self, x, y, neg_sampler):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.neg_sampler = neg_sampler\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.y[idx].shape, self.x[idx, :].shape, next(neg_sampler).shape)\n",
        "        return (self.y[idx], self.x[idx, :], next(neg_sampler))\n",
        "\n",
        "\n",
        "batcher_train = DataLoader(My_Dataset(X, y, neg_sampler), batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxTrC8CY8OJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, embedding_size, vocab_size):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    def forward(self, target_word, context_word, negative_example):\n",
        "        emb_target = self.embeddings_target(target_word)\n",
        "        emb_context = torch.sum(self.embeddings_context(context_word), axis=1)\n",
        "        emb_product = torch.mul(emb_target, emb_context)\n",
        "        emb_product = torch.sum(emb_product, dim=1)\n",
        "        out = torch.sum(F.logsigmoid(emb_product))\n",
        "\n",
        "        emb_negative = self.embeddings_context(negative_example)\n",
        "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
        "        emb_product = torch.sum(emb_product, dim=1)\n",
        "        out += torch.sum(F.logsigmoid(-emb_product))\n",
        "        return -out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKAVm3lVDs9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d178a461-067a-4bbf-b192-56631a70a5fa"
      },
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "net = Word2Vec(embedding_size=200, vocab_size=len(word2index))\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
        "\n",
        "if cuda.is_available():\n",
        "  net = net.cuda()\n",
        "iters=0\n",
        "while True:\n",
        "    losses = []\n",
        "    for batch in batcher_train:\n",
        "        net.zero_grad()\n",
        "        target_tensor, context_tensor, negative_tensor = batch\n",
        "        if cuda.is_available():\n",
        "            target_tensor, context_tensor, negative_tensor = \\\n",
        "                  target_tensor.cuda(), context_tensor.cuda(), negative_tensor.cuda()\n",
        "\n",
        "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.detach().cpu().numpy())\n",
        "        if iters%50==0:\n",
        "          print(\"Loss: \", np.mean(losses))\n",
        "        iters+=1\n",
        "\n",
        "    print(\"Loss: \", np.mean(losses))\n",
        "    early_stopping.update_loss(np.mean(losses))\n",
        "    if early_stopping.stop_training():\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  18782.613\n",
            "Loss:  14256.356\n",
            "Loss:  12972.355\n",
            "Loss:  12106.569\n",
            "Loss:  11565.123\n",
            "Loss:  11179.974\n",
            "Loss:  10834.825\n",
            "Loss:  10550.44\n",
            "Loss:  10399.021\n",
            "Loss:  10265.837\n",
            "Loss:  10029.12\n",
            "Loss:  9815.693\n",
            "Loss:  9602.228\n",
            "Loss:  9397.027\n",
            "Loss:  9253.211\n",
            "Loss:  9110.246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c-qtLNfuEhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}